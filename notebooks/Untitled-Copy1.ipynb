{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.data as data\n",
    "import numpy as np\n",
    "import training_utils as tu\n",
    "from models import CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_text = {\n",
    "    \n",
    "}\n",
    "\n",
    "process_labels = {\n",
    "    \n",
    "}\n",
    "\n",
    "TEXT = data.Field(**process_text)\n",
    "LABEL = data.LabelField(**process_labels)\n",
    "NUM = data.Field(sequential=False)\n",
    "fields = {'category_id': ('label', LABEL), 'lemmatized': ('text', TEXT)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields.update({f'cat_{i}': (f'j{i}',NUM )\n",
    "                                                                                 for i in range(54)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = data.TabularDataset.splits(\n",
    "                                        path = 'data/json',\n",
    "                                        train = 'train.json',\n",
    "                                        validation = 'valid.json',\n",
    "                                        format = 'json',\n",
    "                                        fields = fields\n",
    ")\n",
    "\n",
    "test_data = data.TabularDataset(\n",
    "                                path = 'data/json/test.json',\n",
    "                                format = 'json',\n",
    "                                fields = {'lemmatized': ('text', TEXT)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 25002\n",
      "Number of classes: 54\n"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)\n",
    "print(f'Vocab size: {len(TEXT.vocab)}')\n",
    "print(f'Number of classes: {len(LABEL.vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key = lambda ex: len(ex.text),\n",
    "    sort_within_batch = False,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [2,3,4]\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2606754"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tu.count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer=SummaryWriter('runs/cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.646408083859612 0.8250714869281046 1.3005663739843956 0.6668195709973345\n",
      "Epoch: 1 | Epoch Time: 2m 46:s \n",
      "0.5632608925181082 0.8463541666666666 0.6999461936500654 0.8112388532483942\n",
      "Epoch: 2 | Epoch Time: 2m 49:s \n",
      "0.523290915887048 0.8567197712418301 0.6019869379931795 0.8348984887132771\n",
      "Epoch: 3 | Epoch Time: 2m 49:s \n",
      "0.5072365258789919 0.8630446622963824 0.5572728491670595 0.8457327050347601\n",
      "Epoch: 4 | Epoch Time: 2m 49:s \n",
      "0.5018985830466537 0.8641986655643563 0.525136128684083 0.853390716964814\n",
      "Epoch: 5 | Epoch Time: 2m 49:s \n",
      "0.5010503985986016 0.8650054466101079 0.501704175554278 0.8589810041415767\n",
      "Epoch: 6 | Epoch Time: 2m 49:s \n",
      "0.5033462204282579 0.8656522331284542 0.48441467355997037 0.8629314279727714\n",
      "Epoch: 7 | Epoch Time: 2m 49:s \n",
      "0.5035752951222308 0.8692572167885849 0.46688827204780264 0.8672641265877401\n",
      "Epoch: 8 | Epoch Time: 2m 49:s \n",
      "0.5064624795385825 0.868634259272245 0.4506235413739135 0.8713451567808969\n",
      "Epoch: 9 | Epoch Time: 2m 49:s \n",
      "0.5119308485728659 0.8662037037166894 0.4387682194675879 0.8736635034215627\n",
      "Epoch: 10 | Epoch Time: 2m 49:s \n"
     ]
    }
   ],
   "source": [
    "reload(tu)\n",
    "tu.train_model(model, train_iterator, valid_iterator,\n",
    "            optimizer, criterion, 'cnn', n_epochs=10, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torchtext.data.dataset.Dataset at 0x7f56f50ce320>,\n",
       " <torchtext.data.dataset.Dataset at 0x7f56f50ce668>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
